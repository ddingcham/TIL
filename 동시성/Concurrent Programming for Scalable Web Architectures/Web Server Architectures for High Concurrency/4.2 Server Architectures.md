# Concurrent Programming for Scalable Web Architectures  
# 4. Web Server Architectures for High Concurrency  

## 4.2 Server Architectures   

## Intro  

#### models : merging I/O operations, CPU-bound activities  
* request parsing  
* request handling  

#### more sophisticated variants emerged : combining threads-model and events-model  

## Thread-based Server Architectures  
> basically associates each incoming connection with a separate thread(resp. process)  

#### 동시성을 위한 원자적인 모델이 request 에 초점  
> via synchronous blocking I/O  
> 직관적인 모델  

#### Conceptually, multi-process and multi-threaded architectures share the same principles  
> each new connection is handled by a dedicated activity.  

### Multi-Process Architectures  
* (UNIX-based network server) traditional approach : process-per-connection model  
* isolating different requests promptly, as they do not share memory  
* the creation of processes is a costly operation  
* servers often employ a strategy called preforking  

* ![](http://berb.github.io/diploma-thesis/community/resources/mp-server.svg)  
  * the main server process forks several handler processes (start-up 라이프사이클 이전에)  
  * (thread-safe) socket descriptor is shared among all processes  
  * each process blocks for a new connection,  
  * each process handles the connection and then waits for the next connection  

* heavyweight structure of a process limits the maximum of simultaneous connections  
  > connections-process mapping leads to a concurrency/memory trade-off  

### Multi-Threaded Architectures  
#### New Server Architectures : When reasonable threading libraries have become available  
> replace process with more lightweight threads  

#### thread-per-connection model.  
* **share the same address space and hence share global variables and state**  
* possible to implement mutual features for all request handlers  
  * shared cache for cacheable  
  * but, side-effect  

#### Correct synchronization and coordination is required.  

#### (resource side) advantage : process -> thread  
* a thread only consumes limited memory  
  > compared to the full-blown memory size of an entire process  
* threads require less resources for creation / termination  

#### Acceptor Thread (= Single Dispatcher Thread) - Pool of Worker Threads model   
* ![](http://berb.github.io/diploma-thesis/community/resources/mt-server.svg)  
  * dedicated acceptor  
    * acceptor blocks for new socket connections  
    * acceptor accepts connections  
    * acceptor dispatches them to the worker pool and continues  
  * worker pool / worker threads    
    * worker pool provides a set of threads (worker threads)  
    * worker threads handle incoming request  
    * worker threads are waiting for new reuqests to process  
    
#### Thread pools are a common way of bounding the maximum number of threads inside the server.  

#### Threads from the thread pool take connections from the queue  
* queue is also bounded -> the maximum number of awaiting connections can **be restricted**.  
* **Additional connections will be rejected**.  

#### limits the concurrency : Thread pool and request queue is bounded.  

### Scalability Considerations for Multi-Threaded Architectures  

#### Synchronous, blocking I/O operations  
> In most cases, at least a blocking I/O operation triggers scheduling and causes a context switch  
> see also : datasource -> connection time out  
> 너무 많은 시간이 소요되는 I/O에 대해서는 끊어줘야 동시성을 향상  

multiple CPU cores can be used directly, as threads and rpocesses are scheduled to all cores available.  

#### Under heavy load (more number of threads)  
* multi-threaded web server consumes large amounts of memory  
* constant context switching causes considerable losses of CPU time  
  > indirect penalty : **increased chance of CPU cache misses**  

## Event-driven Server Architectures  
#### Due to the asynchronous/non-blocking call semantics, other models are needed  
> than the prevously outlined thread-per-connection model  

#### mapping of single thread to multiple connections  

#### The thread then handles all occurring events from I/O operations  
> of these connections and requests.  

### Overview : internals of an event-driven architecture  
![](http://berb.github.io/diploma-thesis/community/resources/ev-server.svg)  

#### flow side  
* New events are queued  
* the thread executes a so-called event loop  
* dequeuing events from the queue  
* processing the event  
* taking the next event / waiting for new events to be pushed  

#### single thread model / I/O side  
* A single-threaded event loop consumes event  
  * after event from the queue  
  * sequentially executes associated event handler code  
* New events are emitted by external sources such as socket or file I/O notifications  
* Event handlers trigger I/O actions that eventually result in new events later  

#### UI framework - event/callback  
* 클라이언트의 동작 이벤트에 대한 응답을 (a)synchronous / non-blocking I/O 라고 본다면 ...  
  > asynchronous / synchronous 모두 고려해야 할 케이스  
* **developers using callback** / An application framework can provide an asynchronous interface  
* 똑같은 개념임 (listener - callback - eventHandler)  
* Processing an event requires registered event handler code for specific events, or execution of a callback  

#### the work executed by a thread is very similar to that of a scheduler  
> multiplexing multiple connections to a single flow of execution  
> node.js overview 랑 같이 보면 좋음  

#### control flow of an application following the vent-driven style is somehow inverted.    
* The different states of the connections handled by a thread  
  * are organized in appropriate data structures  
  * either explicitly // data streaming 느낌  
    * using finite state machines (FSM)  
    * implicitly via continuations or closures of callbacks  
* **Instead of sequential operations**  
  * event-driven program uses a **cascade of asynchronous calls and callbacks**  
    > executed on events  
    > 보통 프레임워크 스펙에만 맞추면 알아서 매핑/실행/보장해줌  

#### But, event-driven model often makes the flow of control less obvious and complicates debugging.  
> resolve side effect : **pure function / unit test / immutable object**  

#### high performance event notification interfaces such as epoll and kqueue.  
> via asynchronous / non-blocking  

### Non-blocking I/O Multiplexing Patterns  
* recommending solutions for I/O handling  
  > the problem of network sevices to handle multiple concurrent requests  
  * highly concurrent  
  * high-performance  
* **see also**  
  * [CppCon 2017:Sean bollin \"Reactor vs. Proactor\"](https://www.youtube.com/watch?v=iMRbm32O0ws)  
  * Design Patterns for Distributed Systems  
    > Architecture of Software Systems - Lecture 4  
    > [slide link](https://slideplayer.com/slide/10544081/)  
    * ![](https://slideplayer.com/slide/10544081/36/images/23/Reactor%2FProactor+%E2%80%93+Web+server.jpg)  
    * ![](https://slideplayer.com/slide/10544081/36/images/30/Reactor+vs.+Proactor+Processing+connections+in+web+server.+Reactor+%28left%29+vs.+Proactor+%28right%29.jpg)  
  
#### Reactor Pattern : targets synchronous, non-blocking I/O  
* On startup, an application following this pattern registers a set of     
  * resources (ex] socket)  
  * events (ex] new connection)  
* For each resource event the application is interested in,  
  > an appropriate event handler must be provided a callback or hook method.  
* **The core component of Reactor pattern is a syn chronous event demultiplexer**  
  > that awaits events of resources using blocking event notification interface.  
* Whenever the synchronous event demultiplexer receives an event (ex] a new client connection)  
  > it notifies a **dispatcher and awaits for the next event**  
* The dispatcher processes the event by  
  * **selecting the associated event handler**  
  * **triggering the callback/hoock execution**

* **The Reactor pattern thus decouples a general framework for event handling and multiplexing**  
  > from the application-specific event handlers  
  * The original pattern focuses on a single-threaded execution  
    > This requires the event handlers to adhere to the non-blocking style of operations.  
    > 블로킹 작업이 전체 어플리케이션을 중단하지 않도록  
  * Reactor pattern use a **thread pool for the event handlers**  
  * **additional overhead for coordination and synchronization must be taken into account**  
    > 멀티 코어 플랫폼에서의 성능 향상을 얻는 대신  
  
#### Proactor Pattern : leverages truly asynchronous, non-blocking I/O  
* [POSIX AIO](http://man7.org/linux/man-pages/man7/aio.7.html)  
  > linux system call api  
  > **NOTES** 보기  
  ```
  The POSIX asynchronous I/O (AIO) interface allows applications to initiate one or more I/O operations  
  that performed asynchronously (ex] in the background)  
  application can elect to be notified of completion of I/O  
  -> by [delivery of a signal], [instantiation of a thread], [completion 아니면 no notification at all]  
  ```  
  
* **The Proactor can be considered as an entirely asynchronous variant of the Reactor pattern seen before**  

* incorporates(통합) support for completion events instead of blocking event notification interfaces.  
  * **A proactive initiator represents the main application thread**  
  * **A proactive initiator is responsible for initiating asynchronous I/O operations**  
  * When issuing such an operation, it always registers a completion handler and completion dispatcher  
    > 비동기 연산의 실행은 OS에서 엔티티로 제공하는 비동기 연산프로세서로 관리  
    > asynchronous operation processor  
    
* life cycle 완료 단계에 대한 컨트롤  
  > When the I/O operation has been completed,  
  * the completion dispatcher is notified.  
  * the completion handler processes the result event.  
    
* **확장성 관점에서 Reactor 패턴에 비해 멀티스레딩 지원이 우수**  
  > completion handler 실행이 해당 스레드 풀(dedicated thread pool)로 훨씬 쉽게 전달  

### Scalability Considerations for Event-driven Architectures  

#### Not associating connections and threads -> dramatically dcrease number of threads  
* threads  
  * of the server    
  * the single event-looping thread  
  * some OS kernel threads for I/O  
* 컨텍스트 스위칭에 대한 과도한 오버헤드 방지 가능  
  > **캐시 히트율 증가되니까**  
  > This decreases the memory footprint under load and wastes less CPU time to context switching  
* do not need a thread stack for each connection  
  > **커넥션이 아무리 많아도 스레드 갯수와 커넥션 수 사이에 상관 관계 X**  
  
#### Ideally, the CPU becomes the only apparent bottleneck of an event-driven network application.  
* 리소스가 완전히 포화될 때까지, 이벤트 루프는 처리량 증가에 대해 확장  
* 리소스가 최대로 포화되면, 부하 증가  
  > 포화로 인해, "이벤트 처리를 위한 스레드"로 처리할 수 없는 이벤트는 이벤트큐에 stack up  
* **throughput에 의존하긴 하지만, request latencies의 증가는 선형적으로 일어남**  
  * **acceptable for temporary load peaks**  
    > 일시적으로, 수용 가능하다는 게 큼  
    > 일시적으로 버텨주기만 하면, 그 동안 provisioning 하면 됨  
  * permanent overload degrades performance and renders the service unusable  
    > 확장시킬 시간 줬으니까 ... 대처하셈  

#### 스케쥴링 가능하고, 디커플링된 리소스 관련 이벤트처리 활용  
* 단계 기반으로 접근/분석 한다면?  
* 무조건 필요할 때마다 리소스에 접근하는 게 아니라  
* 여러 개의 리소스 접근 이벤트들을 하나로 묶어서 벌크/배치로 접근 가능  
* 리소스에 대한 I/O 횟수를 줄일 수 있음  
* 가져 온 다음에는 다시 풀어서 나눠줘야 함 / map/reduce 개념 적용  
* 나눠줄 대상들 중 중복되는 대상이 있을 수 있으므로 Immutable 하게  
  
#### event-driven / multi-core  
* **thread-based model**은 I/O 동시성과 CPU 동시성에 모두 고려해야 함  
* **event-driven model**은 I/O 동시성만 고려하면 됨  

#### multiple CPUs/cores 활용을 위한 event-driven servers 고려 사항  
* **N-copy approach : instantiation of multiple separate server processes on a single machine**  
  > using N instances on a host with N CPUs/cores  
  * 머신(시스템) 내에 load balancer를 통해서 서버 인스턴스들에 부하 배분  
  * **N 개의 서버 인스턴스들이 서버 소켓을 공유하게 되므로 이에 대한 처리가 필요**    

* ex] implementation of this approach : node.js / cluster module  
  > [node.js cluster doc](http://nodejs.org/docs/latest/api/cluster.html)  
  > 복수의 어플리케이션 인스턴스들을 fork  
  > 서버소켓 하나로 공유하는 개념  
  > N-copy approach  

#### stateless / shared-nothing components  
> 내부 캐시 사용을 위한 필요한 서버 설계 변경  
  >> 사전 조건 1) single-threaded server   
  >> 사전 조건 2) sequential execution semantics of callbakcs  
  
* 사전 조건에 따라 single event-driven application 내부에 복수의 프로세스 사용 불가  

* 주로 **libasync-smp** 활용  
  > 여러 프로세스와 병렬 콜백 실행에 도움을 주는 비동기 프로그래밍 라이브러리  
  > ZELDOVICH, Nickolai; YIP, Er; DABEK, Frank; MORRIS, Robert T.; MAZIÃ¨RES, David KAASHOEK, Frans:  
  > Multiprocessor support for event-driven programs, :  
  > In Proceedings of the 2003 USENIX Annual Technical Conference (USENIX â€™03, 239-252  

  * 프로그래밍 모델은 **simple sequential programming model** 유지  
  * 토큰 개념 적용  
    > tokens : so-called colers assigned to each callback  
  * Callback with different colors(token) **can be executed in parallel**  
  * 모든 콜백에 대해 default color를 적용하면, 기존 방식도 가능  
  
* 캐시를 쓰는 웹 서버에 **coloring 개념** 적용 // 추가적인 동시성을 위해  
  * reading and parsing requests phase   
    > 새로운 요청을 읽고, 파싱하는 작업은 sequential  
    > 각기 다른 요청들에 대해서는 동시에 처리 가능  
    * 각각의 요청들을 coloring  
    * 같은 파싱 동작은 같은 coloring / 다른 파싱 동작은 다른 coloring  
    * 색이 다른 애들에 대해서 병렬로 처리 가능  
  
  * after parsing phase  
    * the server must check if the required content is already cached  
    * cache 에 대한 키 소스 중 하나로 color 활용  
    * 있으면 from cache    
    * 없으면 from the application server  
  
* **일관성 유지를 위해 캐시를 확인하는 작업은 순차적으로 일어나야 함** // 트랜잭셔널하게, 원자적으로  
  > 어떤 키에 대한 결과를 캐시에 올리는 도중에, 해당 키로 접근하면 캐시에 올리는 걸 또 해야되니까  
  > indicating the scheduler to run **all of these operations always serially**  

* **this library also allows the callback to execute partially blocking operations**  
  > async/await 랑 비슷한 듯  
  
* 결국 동시성 해결을 위한 풀이법의 기본 개념은 비슷한 거 또 느낌  
  > 락 걸 지 말지 여부 / 어디 까지 락을 걸지    
  > context에 맞춰서만 하면 됨  

#### 이벤트 주도 모델의 단점은, 실행 플로우에 대해 정의하는 게 어렵고, 복잡하다는 거     

