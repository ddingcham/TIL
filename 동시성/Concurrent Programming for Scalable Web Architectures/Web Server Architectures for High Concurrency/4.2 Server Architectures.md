# Concurrent Programming for Scalable Web Architectures  
# 4. Web Server Architectures for High Concurrency  

## 4.2 Server Architectures   

## Intro  

#### models : merging I/O operations, CPU-bound activities  
* request parsing  
* request handling  

#### more sophisticated variants emerged : combining threads-model and events-model  

## Thread-based Server Architectures  
> basically associates each incoming connection with a separate thread(resp. process)  

#### 동시성을 위한 원자적인 모델이 request 에 초점  
> via synchronous blocking I/O  
> 직관적인 모델  

#### Conceptually, multi-process and multi-threaded architectures share the same principles  
> each new connection is handled by a dedicated activity.  

### Multi-Process Architectures  
* (UNIX-based network server) traditional approach : process-per-connection model  
* isolating different requests promptly, as they do not share memory  
* the creation of processes is a costly operation  
* servers often employ a strategy called preforking  

* ![](http://berb.github.io/diploma-thesis/community/resources/mp-server.svg)  
  * the main server process forks several handler processes (start-up 라이프사이클 이전에)  
  * (thread-safe) socket descriptor is shared among all processes  
  * each process blocks for a new connection,  
  * each process handles the connection and then waits for the next connection  

* heavyweight structure of a process limits the maximum of simultaneous connections  
  > connections-process mapping leads to a concurrency/memory trade-off  

### Multi-Threaded Architectures  
#### New Server Architectures : When reasonable threading libraries have become available  
> replace process with more lightweight threads  

#### thread-per-connection model.  
* **share the same address space and hence share global variables and state**  
* possible to implement mutual features for all request handlers  
  * shared cache for cacheable  
  * but, side-effect  

#### Correct synchronization and coordination is required.  

#### (resource side) advantage : process -> thread  
* a thread only consumes limited memory  
  > compared to the full-blown memory size of an entire process  
* threads require less resources for creation / termination  

#### Acceptor Thread (= Single Dispatcher Thread) - Pool of Worker Threads model   
* ![](http://berb.github.io/diploma-thesis/community/resources/mt-server.svg)  
  * dedicated acceptor  
    * acceptor blocks for new socket connections  
    * acceptor accepts connections  
    * acceptor dispatches them to the worker pool and continues  
  * worker pool / worker threads    
    * worker pool provides a set of threads (worker threads)  
    * worker threads handle incoming request  
    * worker threads are waiting for new reuqests to process  
    
#### Thread pools are a common way of bounding the maximum number of threads inside the server.  

#### Threads from the thread pool take connections from the queue  
* queue is also bounded -> the maximum number of awaiting connections can **be restricted**.  
* **Additional connections will be rejected**.  

#### limits the concurrency : Thread pool and request queue is bounded.  

### Scalability Considerations for Multi-Threaded Architectures  

#### Synchronous, blocking I/O operations  
> In most cases, at least a blocking I/O operation triggers scheduling and causes a context switch  
> see also : datasource -> connection time out  
> 너무 많은 시간이 소요되는 I/O에 대해서는 끊어줘야 동시성을 향상  

multiple CPU cores can be used directly, as threads and rpocesses are scheduled to all cores available.  

#### Under heavy load (more number of threads)  
* multi-threaded web server consumes large amounts of memory  
* constant context switching causes considerable losses of CPU time  
  > indirect penalty : **increased chance of CPU cache misses**  

## Event-driven Server Architectures  


### Non-blocking I/O Multiplexing Patterns  

### Scalability Considerations for Event-driven Architectures  



