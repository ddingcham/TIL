# Concurrent Programming for Scalable Web Architectures  
# 6. Concurrent and Scalable Storage Backends  

## 6.2 Internals of Distributed Database Systems  

### Building Blocks  

* Traditional database internals  
  * indexes and data structures  
  * I/O handling components  
  * transaction handling  
  * concurrency control  
  * query processing  
  * client communication interfaces  
  
* necessary building blocks for distributed database systems  
  * transaction management  
  * concurrency control  
  * data versioning  
  * interfaces  
  * scalable data partitioning  
  * parallel data processing  
  
#### Distributed Transaction Management  
> see also : [ibm : 분산 트랜잭션 없이 사용하기](https://developer.ibm.com/kr/developer-%EA%B8%B0%EC%88%A0-%ED%8F%AC%EB%9F%BC/2017/08/14/%EB%B6%84%EC%82%B0-%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98-%EC%97%86%EC%9D%B4-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/)    
    
* Distributed transactions handle operations with transactional behavior **between multiple nodes**  
  * Applying to all participating nodes or to no one at all  
  * (partial) failures of nodes and non-locality  
    > more difficult to implement due to the risk of network errors  
    
* basic component -> coordinating service  
  > managing and coordinating **transactions between all participants**  
  > based on a transaction protocol  
  * popular transaction protocols  
    * ![](https://developer.ibm.com/kr/wp-content/uploads/sites/98/04.jpg)  
      > see also : [ibm : 트랜잭션의 기초 및 분산 트랜잭션](https://developer.ibm.com/kr/cloud/2017/08/11/%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98%EC%9D%98-%EA%B8%B0%EC%B4%88-%EB%B0%8F-%EB%B6%84%EC%82%B0-%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98/)  
    * 2 Phase Commit : voting phase + completion phase  
      > blocking and not fault-tolerant  
    * 3 Phase Commit : 2 Phase + coordination steps  
      > cannot cope with network partitions  
    * [quorum-based voting protocols](https://www.revolvy.com/page/Quorum-%28distributed-computing%29) can be used for committing transactions in ditributed setups  
      > 잘 안 와닿 ....  
      * marking a transaction as executed, when the majority of nodes have executed it  
      * either the abort quorum or the commit quorum must be obtained for termination  
        
 
#### Concurrency Control and Data Versioning  
* inherent parallelism state a problem  
  * when concurrent write operations are allowed on different nodes  
  * relaxed consistency guarantees and the acceptance of network partitions require  
    * concepts for data versioning  
    * concepts for controlling concurrent operations on data  
    
* distributed concurrency control mechanisms  
  * pessimistic argorithms : optimistic argorithms  
    * pessimistic  
      * providing conflict prevention  
        > by strcit coordination of concurrent transactions  
    * optimistic  
      * not expecting regular conflicts  
      * delaying conflict checking to the end of a transaction life-cycle  
  * locking-based algorithms : timestamp-based algorithms : hybrid algorithms  
    * locking-based  
      * using explicit locks for operations in order to prevent conflicts  
      * traditional [two-phase-locking : 2PL](http://www.jidum.com/jidums/view.do?jidumId=284)  
      * quorum-based voting using read and write quorums    
    * timestamp-based  
      > via [MVCC](https://vladmihalcea.com/how-does-mvcc-multi-version-concurrency-control-work/)  
      > how does mvcc multi version concurrency control work  
      * similar implementations of MVCC for STM systems  
      * using logical clocks to identify either data changes or transactions over time  
        * **logical ordering**  
          > allowing to reason about the sequence of operations  
          > allowing to protect from conflicts  
 
#### Data Interfaces and APIs  
 > For distributed database systems, **interfaces for remote access are obligatory**  
* Software components such as JDBC or ODBC  
  * abstracting from concrete databse protocols  
  * supplying generic APIs  
   
* established distributed technologies  
  > these technologies facilitate application integration and testing  
  * communication  
    > dispatching Database calls and queries  
    * via RPC invocations : [Apache-Thrift](https://github.com/apache/thrift)    
    * via HTTP requests  
    
  * data serialization  
    > interchanging Data using serialization technologies  
    * based on RPC  
      * via Thrift's own serialization  
      * via Google's Protocol Buffers  
    * based on HTTP  
      > often emulate some of the REST principles  
      * for serialization : JSON / BSON (binary JSON) / XML  
      
* comparance  
  * low-level RPC calls generally provide a **slightly better performance** due to less overhead  
  * HTTP-based APIs introduces HTTP concepts like **caching for free**  
  
#### Scalable Data Partitioning  
* Consistent hashing maps nodes and data items into the same ring for partitioning  
  * ![](http://berb.github.io/diploma-thesis/community/resources/cons_hash.svg)  
  * left illustration shows a set of data items (gray) mapped to three nodes  
  * right illustration shows the additional Node D has joined the system  
  * **as a result, only a small sector of the ring is affected from repartitioning**  
    > Node D takes over two data items that have formerly been assigned to Node A  
    
* Allocating large amounts of data to a number of nodes -> more complex  
  > data scalability & number of available nodes changes  
  * Scaling out means supplying additional nodes / **often at runtime in the first place**  
  * Scaling back to less node is interesing, when the amount of data decreases  
  * requirements for strategies  
    * how to partition  
    * how to allocate data when scaling in and out  
    
* straightforward strategy : fixed number of hosts  
  * allocating data by applying a hash function on a data item (key)  
  * using the result **modulo** the number of nodes  
    > in order to calculate the node responsible for the item  
  * **but, it fails when the number of nodes changes**  
  
* recalculating and redistributing all items due to changed partitioning keys is then necessary  
  > but not reasonable in practice  
  
* **Consistent Hashing** feat. Consistent hashing maps nodes and data items into the same ring for partitioning   
  * fundamental idea : to hash data items and nodes into a common ring using the same hash function  
  * each data item has to be stored by **the next clockwise adjacent node** in the ring   
  * when new nodes are added to the ring or nodes leave the ring  
    * **a small sector of the ring is affected**  
    * **only the data items in this sector must be reallocated**  
  * working with varying number of nodes and provides a consistent mapping  
    **that prevents an unnecessary reallocating of data when the amount of nodes scales**  

#### Parallel Data Processing  
* Generating indexes requires the execution of the same operations on all data entries and machines  
  * In several non-relational database systems, it is the developer's task to implement index generating  
  * **hence appropriate programming models are required for such "embarrassingly parallel" tasks**  
  
* **MapReduce model**    
  > popular approach  
  * ![](http://berb.github.io/diploma-thesis/community/resources/mapreduce.svg)  
  * Separating parallel processing of possibly large data sets  
    * **map** function  
      * taking data entries  
      * emitting **intermediate key-value pairs**  
      * all intermediate pairs are **grouped by keys**  
    * **reduce** function  
      * applying to all intermediate pairs with the same key, **yielding simple values as a result**  
  * Distribution & Coordination & Execution is managed by the framework-resp-databaseSystem  
    > developer only provides the map and reduce function  
    > cloud 벤더에서 지원하는 serverless lambda 제품 보면 진짜 미쳤음  
  * MapReduce is also used for  
    * building indexes  
    * either using the sorted intermediate key-value pairs  
    * using the sorted reduced results  


