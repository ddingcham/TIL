# Concurrent Programming for Scalable Web Architectures  
# 5. Concurrency Concepts for Applications and Business Logic  

## 5.2 Concurrency Based on Threads, Locks and Shared State  

#### Imperative programming is built around the notion of sequential execution and mutable state.  
> the most popular form of structured programming  
> fromt the conceptions of the Von Neumann architecture  

#### threads are main constructs for parallelism provided by OS and hardware architecture (hyperthreading)  
> threads are the prevailing building blocks for concurrency  

#### However, concurrent programming based on threads, locks and shared state is said to be difficult and error-prone

### The Implications of Shared and Mutable State  

* **Unlike processes, threads share the same address space.**  
  * multiple independent threads may access the same variable and states concurrently  
  * multiple threads may compete for write operations, too  
    > sequential programming is built on the concept of mutable state  
  * multithreading is used with preemptive scheduling  
    > the exact switches and interleavings betwenn multiple threads are not known in advance  
    * **strong form of indeterminacy**  
      > mutable state and indeterminacy introduce the strong hazard of race conditions  
    
* **A race condition occurs when two or more thread compete for access to critical section**  
  * section : contains state shared between threads  
  * the race condition may result in various inconsistent states  
  * a thread may read stale state while another thread is already updating it  
    > multiple threads alter the state at the same time  
    > either one of the changes may last and the others get lost  
  
* **we need mechanisms to guard critical sections and enforce synchronized access**  

### Locking Mechanisms  
> general primitives for synchronization  
> control access to critical sections  

#### behaviors & semantics side : different types of locks  

#### Semaphores  
> simple locks that provide a **wait** and **signal** function  
* **binary semaphore**  
  * Before entering a critical section or using a shared resource  
    > the **wait** function must be called.  
    > 내가먼저 wait 걸었으니, wait 동안 쓰지 마셈  
  * Once the critical section has been traversed,  
    > it is freed using **signal**  
    > 다 썼음, 이제 써도 됨  
  
* **counting semaphores**  
  * allow a bounded number of threads to pass  
  * bounded number 가 1 이면, binary semaphore 임  
  
* **ownership**  
  > critical section is temporarily possessed by a distinct thread  
  > a distinct thread : **the only instance able to unlock it later**  
  * 어떤 critical section에 대해 ownership을 가진 채로 thread가 죽으면 ?  
    > [What will happen if a thread dies inside the critical section?
](https://stackoverflow.com/questions/19069877/what-will-happen-if-a-thread-dies-inside-the-critical-section)    
    * 1) Do you mean that if faults through an incorrect memory or other access ?  
      > 프로세스 전체가 OS에 의해 종료 (프로세스 플로우 자체가 종료)  
    * 2) Do you mean that the thread is terminated (자체적인 흐름 종료 / 다른 스레드에 의해 종료)  
      > all threads waiting on the critical section will be stuck forever  
    * 3) 스레드 내에서의 무한 루프 (OS 의존 / 기본적으로 두 가지 전략)  
      > (1) "everything appears to be running correctly so I should stay away"  
      > (2) "이로 인해 프로세스가 리소스 최대 할당량(CPU)을 초과하면 OS가 프로세스 자체를 종료 시킴"  
        >> 단일한 특정 스레드만을 종료할 수 있는 건 프로세스 레벨에서만 가능  
      
* **monitor**  
  > modulary protects sections using **condition variables**  
  * The internal **condition variable** allows a  
  * **blocking thread** to yield temporarily  
  * **blocking thread** to wait for a modification of the condition triggered by other threads  
  
* **reentrancy**  
  > a thread that has already obtained a certain lock can pass the lock again  
  > thread 내에서 critical section을 사용하는 재귀 호출 생각  
  > that may repeatedly access critical sections  
  
* **Read / write locks**  
  * via. counting locks  
    > lock 갯수 뿐만이 아니라, 액세스 관점에서 lock을 구별  
  * shared access for reading threads  
  * exclusive access for threads that demand write access  
  

#### The Consequences of Locking  
* when obtained locks are not released or locks to acquire never become available  
  > actually break the application  

* Higher-level abstractions like monitors often provide means  
  * to mark entire sections of code for mutual exclusion  
  * to implicitly acquire and release lock  
  * **still fall victim to locking issues**  
  
* **deadlock**  
  * two or more threads compete for locks with **cyclic dependencies**  
  * locks for two or more critical sections depended by two or more threads  
  * 서로 상대게 필요해서 락을 반환 안 하는건데 ....  
  
* other locking issues  
  * livelocks  
    * prevent threads to continue  
    * appears when two threads simultaneously start to claim a first resource, then restart again    
      > 이를 계속 반복  

* solution  
  * starvation issues such as livelocks might be handled at runtime using random backoffs for retires  
    > potential locking issues are generally very difficult to detect due to nondeterminism  
  * risk of a deadlock increases when multiple locks with fine granularities are used  
    > **the use of coarse locks is often recommended in order to avoid deadlocks**  
    > coarse locks result in an eventually sequential execution of threads  
    > **contrary to increased parallelism**  
    
* see also : https://wonjayk.tistory.com/251  


  

  

### Multithreading and Locks for Concurrent Application Logic  

