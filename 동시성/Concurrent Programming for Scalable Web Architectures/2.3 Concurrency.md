# Concurrent Programming for Scalable Web Architectures
# 2.3 Concurrency  
> Concurrency  
> is a property of a system representing the fact  
> that multiple activities can be executed at the same time.  

## Intro  

#### "several independent activities, each of which executes at its own pace"
> ROY, Peter Van HARIDI, Seif: Concepts, Techniques, and Models of Computer Programming, The MIT Press (2004)  

#### execute environments  
> (특수한 환경은 없고 -> 컨셉은 같음)  
> 수준/레벨에 따라 대상만 바꾸면 됨  
* single-core processors  
* multi-core processors  
* multi-processors  
* multiple machines  
  > as part of a distributed system.  
  
#### three fundamental ways  
> multi-core and multiprocessor systems  
> how the concurrent execution of an application can improve its performance  
> CANTRILL, Bryan BONWICK, Jeff: Real-World Concurrency. Queue (2008), 6: 16-25  

* **Reduce latency**  
  > **shorter time**  
  > 작업 단위를 동시에 실행 가능할 만큼 쪼개기  

* **Hide latency**  
  > **blocked tasks** (external resources)  
  > long-running tasks 모아서 batch 컨셉으로 처리  
  > 갔다 오는게 오래 걸리니까, 여러 개 묶어서 한 번 갔다오도록  

* **Increase throughput**  
  > **independent sequential tasks**  
  > By executing multiple tasks concurrently,  
  > the general system throughput can be increased.  
  * 꼭 동시성을 고려한 디자인이 아니더라도, 로직 자체의 효율성도 당연히 중요  
    > ex] transaction isolation level 설정 같은 거  
  
#### 동시성은 모든 분산 시스템의 본질적인 속성  
#### All web applications can be used by various users at the same time.  
> 클라이언트 연결 뿐만 아니라 외부 리소스와의 연결, 비즈니스 로직 역시 동시성 이슈와 연관    

## Concurrency and Parallelism  
> Somtimes both terms are even used synonymously.  

### Concurrency vs. Parallelism  
> Concurrency : conceptual property of a program   
> Parallelism : runtime state  

* Dependency  
  * Concurrency of a program  
    > programming language.  
    > way it is coded.  
  * Parallelism  
    > actual runtime environment.  

#### concurrent execution ?  
> N tasks may be performed sequentially, alternately, or even simultaneously.  
* Scheduling side  
  * like multi-core or multi-processor systems  
    > true parallelism can only be achieved  
    > **if the hardware architecture supports parallel execution**.  
  * single-core machine  
    > execute multiple threads concurrently  
    > **however it can never provide true parallelism**.  
    
### Concurrent Programming vs. Parallel Programming  
> Differentiating concurrent and parallel programming is more tedious(지루한).  
> 윗 단계의 골은 같지만,(수준/레벨에 따라 대상만 바꾸면 됨)  
> 실질적인 문제 해결을 위해서는 각각 다른 컨셉의 문제를 다뤄야 하므로 이해/지식이 필요함.  

#### Concurrent Programming side : Reducing and Hiding Latency  
> 비결정론적인 흐름 제어(non deterministic control flow)는  
> 복잡함을 야기  
> Concurrent programming tackles(help 느낌) concurrent and interleaving tasks  
> 그리고 위에 야기된 복잡함  

**동시 처리량(throughtput) 자체를 늘리는 것 만큼이나 latency를 줄이거나 감추는 것에 초점**  
> 대기시간이 짧으면 짧을 수록 **동시**에 가까워짐  

#### Parallel Programming side : deterministic control flow  
> 결정론적인 흐름 제어와 throughput 최적화  
> 대역폭 자체를 효율적으로 쓰자  
> ex] 제한 된 갯수의 thread들이 쉬지 않고 사용되도록 (닫히지 않는 stream 스럽게)    

#### Non Deterministic/Deterministic control flow  
> **문제(작업)를 어떤 추상화의 기준으로 봐라보고, 나눌 건지 (?)**
> Difference between Deterministic and Non-deterministic Algorithms  
> https://www.geeksforgeeks.org/difference-between-deterministic-and-non-deterministic-algorithms/

* NonDeterministic control flow (결정된 게 없음 타면 타는 거고 ... )  
  ```
  Some of the terms related to the non-deterministic algorithm are defined below:

  choice(X) : chooses any value randomly from the set X.
  failure() : denotes the unsuccessful solution.
  success() : Solution is successful and current thread terminates.
  ```
  * 각각의 입력에 대해 다른 실행과 다른 출력  
    > 특정 입력에 다른 솔루션을 거칠 수 있음  
  * **작업 실행 시간을 다항식 형태로 표기 불가**  
  * 다음 작업에 대한 결정을 할 수 없음  
    > 로직 흐름의 경로가 하나 이상 존재  
    
* Deterministic control flow (상태 베이스로 어떤 흐름을 타게될 지 결정/예측 가능)  
  ```
  In deterministic algorithm, 
  for a given particular input, 
  
  the computer will always produce the same output going through the same states
  ```
  > 각각의 입력에 대해 같은 출력 보장  
  > **작업 실행 시간을 다항식 형태로 표현 가능**  
  > 상태를 보고, 다음 작업에 대한 결정 가능  
    > 로직 흐름의 경로를 하나로 판단 가능  
    
#### 병렬 프로그래밍 추상화의 좋은 예 : MapReduce(Google) / ForkJoin(Java)   
> 병렬 패러다임은 고성능 연산, 그래픽(수많은 픽셀 연산) 처리 에 많이 사용됨  
 
* 웹과 동시성  
> 일반적으로 웹서버 내부는 동시 프로그래밍(Concurrent programming)의 전형적인 예    
> 하나의 작업 (웹 요청) 당 하나의 thread (control flow)  
  >> 맞나(?) http message 상태값 베이스로 어떤 흐름을 타게 될지 예측 가능  
  >> Deterministic control flow 의 특징  
   
 
## Computer Architectures Supporting Concurrency  
> 명령의 동시 처리 가능 여부 / 데이터 스트림의 단/복수 베이스로 분류  
 
#### Non-real Parallelism
  * SISD (Single-Instruction, Single-Data)  
    > 전통적인 단일 프로세서 머신  
    > lack of physical concurrency (thread/스케쥴링 개념으로 동시 처리 흉내 가능)  
  * MISD (Multiple-Instruction, Single-Data)  
    > data stream 하난데 명령 동시 처리를 어따 씀?  
    > 예외 케이스임  
    > **fault-tolerant** computing 을 위해서 중복 실행이 필요한 경우에 쓰임  
      >> 명령 실행 하나하나가 크리티컬한 분야가 뭐 있는지 ?  
      >> 일단 동시성 관련된 건 아니네  
     
### Real Parallelism  
> Multiple Data Stream 을 지원할 때만 Real Parallelism 이라고 볼 수 있음  

* SIMD (Single-Instruction, Multiple-Data)  
  > 그래픽 처리 / 벡터 프로세싱 (연산 자체는 짜잘이고, 반영 해야 될 곳이 여러 곳)  
  > 이 구조 역시 웹에는 어울리지 않음  
  
#### MIMD (Multiple-Instruction, Multiple-Data)  
> Scalable Web Architectures를 위해 우리가 필요로 하던 모습  
> 공유(global) 메모리 / 분산(distributed) 메모리 / multiple cpu|core|machine  
> large, scalable 검색하면 많이 나오는 아키텍쳐들이랑 비슷한 구조  

## Model

  
