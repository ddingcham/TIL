# Concurrent Programming for Scalable Web Architectures
# 2.3 Concurrency  
> Concurrency  
> is a property of a system representing the fact  
> that multiple activities can be executed at the same time.  

## Intro  

#### "several independent activities, each of which executes at its own pace"
> ROY, Peter Van HARIDI, Seif: Concepts, Techniques, and Models of Computer Programming, The MIT Press (2004)  

#### execute environments  
> (특수한 환경은 없고 -> 컨셉은 같음)  
> 수준/레벨에 따라 대상만 바꾸면 됨  
* single-core processors  
* multi-core processors  
* multi-processors  
* multiple machines  
  > as part of a distributed system.  
  
#### three fundamental ways  
> multi-core and multiprocessor systems  
> how the concurrent execution of an application can improve its performance  
> CANTRILL, Bryan BONWICK, Jeff: Real-World Concurrency. Queue (2008), 6: 16-25  

* **Reduce latency**  
  > **shorter time**  
  > 작업 단위를 동시에 실행 가능할 만큼 쪼개기  

* **Hide latency**  
  > **blocked tasks** (external resources)  
  > long-running tasks 모아서 batch 컨셉으로 처리  
  > 갔다 오는게 오래 걸리니까, 여러 개 묶어서 한 번 갔다오도록  

* **Increase throughput**  
  > **independent sequential tasks**  
  > By executing multiple tasks concurrently,  
  > the general system throughput can be increased.  
  * 꼭 동시성을 고려한 디자인이 아니더라도, 로직 자체의 효율성도 당연히 중요  
    > ex] transaction isolation level 설정 같은 거  
  
#### 동시성은 모든 분산 시스템의 본질적인 속성  
#### All web applications can be used by various users at the same time.  
> 클라이언트 연결 뿐만 아니라 외부 리소스와의 연결, 비즈니스 로직 역시 동시성 이슈와 연관    

## Concurrency and Parallelism  
> Somtimes both terms are even used synonymously.  

### Concurrency vs. Parallelism  
> Concurrency : conceptual property of a program   
> Parallelism : runtime state  

* Dependency  
  * Concurrency of a program  
    > programming language.  
    > way it is coded.  
  * Parallelism  
    > actual runtime environment.  

#### concurrent execution ?  
> N tasks may be performed sequentially, alternately, or even simultaneously.  
* Scheduling side  
  * like multi-core or multi-processor systems  
    > true parallelism can only be achieved  
    > **if the hardware architecture supports parallel execution**.  
  * single-core machine  
    > execute multiple threads concurrently  
    > **however it can never provide true parallelism**.  
    
### Concurrent Programming vs. Parallel Programming  
> Differentiating concurrent and parallel programming is more tedious(지루한).  
> 윗 단계의 골은 같지만,(수준/레벨에 따라 대상만 바꾸면 됨)  
> 실질적인 문제 해결을 위해서는 각각 다른 컨셉의 문제를 다뤄야 하므로 이해/지식이 필요함.  

#### Concurrent Programming side : Reducing and Hiding Latency  
> 비결정론적인 흐름 제어(non deterministic control flow)는  
> 복잡함을 야기  
> Concurrent programming tackles(help 느낌) concurrent and interleaving tasks  
> 그리고 위에 야기된 복잡함  

**동시 처리량(throughtput) 자체를 늘리는 것 만큼이나 latency를 줄이거나 감추는 것에 초점**  
> 대기시간이 짧으면 짧을 수록 **동시**에 가까워짐  

#### Parallel Programming side : deterministic control flow  
> 결정론적인 흐름 제어와 throughput 최적화  
> 대역폭 자체를 효율적으로 쓰자  
> ex] 제한 된 갯수의 thread들이 쉬지 않고 사용되도록 (닫히지 않는 stream 스럽게)    

#### Non Deterministic/Deterministic control flow  
> https://kamang-it.tistory.com/entry/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%ED%8C%A8%EB%9F%AC%EB%8B%A4%EC%9E%84%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EA%B8%B0%EB%B0%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8DEvent-based-programming?category=693872  
> **문제(작업)를 어떤 추상화의 기준으로 봐라보고, 나눌 건지 (?)**  
> Difference between Deterministic and Non-deterministic Algorithms  
> https://www.geeksforgeeks.org/difference-between-deterministic-and-non-deterministic-algorithms/  

* NonDeterministic control flow (결정된 게 없음 이벤트가 발생하면 타는 거고 ... )  
  ```
  Some of the terms related to the non-deterministic algorithm are defined below:

  choice(X) : chooses any value randomly from the set X.
  failure() : denotes the unsuccessful solution.
  success() : Solution is successful and current thread terminates.
  ```
  * 각각의 입력에 대해 다른 실행과 다른 출력  
    > 특정 입력에 다른 솔루션을 거칠 수 있음  
  * **작업 실행 시간을 다항식 형태로 표기 불가**  
  * 다음 작업에 대한 결정을 할 수 없음  
    > 로직 흐름의 경로가 하나 이상 존재  
    
* Deterministic control flow (상태 베이스로 어떤 흐름을 타게될 지 결정/예측 가능)  
  ```
  In deterministic algorithm, 
  for a given particular input, 
  
  the computer will always produce the same output going through the same states
  ```
  > 각각의 입력에 대해 같은 출력 보장  
  > **작업 실행 시간을 다항식 형태로 표현 가능**  
  > 상태를 보고, 다음 작업에 대한 결정 가능  
    > 로직 흐름의 경로를 하나로 판단 가능  
    
#### 병렬 프로그래밍 추상화의 좋은 예 : MapReduce(Google) / ForkJoin(Java)   
> 병렬 패러다임은 고성능 연산, 그래픽(수많은 픽셀 연산) 처리 에 많이 사용됨  
 
* 웹과 동시성  
> 일반적으로 웹서버 내부는 동시 프로그래밍(Concurrent programming)의 전형적인 예    
> 하나의 작업 (웹 요청) 당 하나의 thread (control flow)  
  >> 맞나(?) http message 상태값 베이스로 어떤 흐름을 타게 될지 예측 가능  
  >> Deterministic control flow 의 특징  
   
 
## Computer Architectures Supporting Concurrency  
> 명령의 동시 처리 가능 여부 / 데이터 스트림의 단/복수 베이스로 분류  
 
#### Non-real Parallelism
  * SISD (Single-Instruction, Single-Data)  
    > 전통적인 단일 프로세서 머신  
    > lack of physical concurrency (thread/스케쥴링 개념으로 동시 처리 흉내 가능)  
  * MISD (Multiple-Instruction, Single-Data)  
    > data stream 하난데 명령 동시 처리를 어따 씀?  
    > 예외 케이스임  
    > **fault-tolerant** computing 을 위해서 중복 실행이 필요한 경우에 쓰임  
      >> 명령 실행 하나하나가 크리티컬한 분야가 뭐 있는지 ?  
      >> 일단 웹에서 고려할 동시성 관련된 건 아니네  
     
### Real Parallelism  
> Multiple Data Stream 을 지원할 때만 Real Parallelism 이라고 볼 수 있음  

* SIMD (Single-Instruction, Multiple-Data)  
  > 그래픽 처리 / 벡터 프로세싱 (연산 자체는 짜잘이고, 반영 해야 될 곳이 여러 곳)  
  > 이 구조 역시 웹에는 어울리지 않음  
  
#### MIMD (Multiple-Instruction, Multiple-Data)  
> Scalable Web Architectures를 위해 우리가 필요로 하던 모습  
> 공유(global) 메모리 / 분산(distributed) 메모리 / multiple cpu|core|machine  
> large, scalable 검색하면 많이 나오는 아키텍쳐들이랑 비슷한 구조  

## Models for Programming Concurrency  
> four main approaches for programming concurrency  
> ROY, Peter Van HARIDI, Seif: Concepts, Techniques, and Models of Computer Programming, The MIT Press (2004)  
> 동시성 관련 웹 아키텍쳐 솔루션에 대한 잠재적인 개념들  

### Models for Programming Concurrency
> _four main approaches for programming concurrency  
* Concurrency  
  * Sequential  
  * Declarative  
  * Message-passing  
  * Shared-state  

#### Sequential Programming (순차적 프로그래밍)  
* 결정론적인 프로그래밍 모델, 동시성과는 전혀 상관 없음.  
* 이 모델이 강해질 경우 : 프로그램의 전체 작업들에 대해 모두 순서화  
* 이 모델이 약해질 경우 : 결정론적 프로그래밍의 특징은 유지하지만,  
  * **정확한 실행 순서**를 보장하진 않음  
  * 현재 활성화된 작업 처리에 대한 보장은 제공 (하긴 해 / 루틴들이 순서에 평등한 느낌?)  

#### Declarative Concurrency (선언적 동시성)  
> Declarative Programming(선언적 프로그래밍) : **implicit control flow**(암시적인 제어 흐름)  
> 처리 순서를 직접적으로 설명 하지는 않지만,  
  >> 각각의 컴퓨팅적 논리(책임)들을 합치는 모델  
* 선언적 프로그래밍 모델에 복수의 실행 흐름을 추가하면, declarative concurrency로 확장  
* data-driven / demand-driven 접근 방식 기반의 동시성을 암시적으로 추가  
  > 데이터를 옵저빙 -> 관찰하던 데이터가 바뀌면 그에 반응    
* While this introduces some form of nondeterminism at runtime,  
  > 런타임 시점에 비결정론적인 요소가 결정됨  
* 일반적으로 비결정론적인 요소는 외부에서는 알 수 없음  
  > **추상화된 걸 참조하는 클라이언트가 실 구현체를 모르게 하듯이**  

#### Message-passing Concurrency  
> 메시지를 통해서 concurrent activities 들이 통신  
* 각 message 가 사팩 없는 pure function 면 좋겟지.  
  > Generally, this is the only allowed form of interaction  
  > between activities which are **otherwise completely isolated**.  
* synchronous / asynchronous 한 메시지냐에 따라 동기화를 위한 패턴이나 메커니즘이 달라짐  
  > sync - sync / async - async 는 큰 문제가 없겠지만,  
  > sync - async 케이스는 **항상 중간에 뭔가 조작/조작해주는 애 가 필요했음**.  

#### Shared-state Concurrency  
> multiple activities 들이 **경합 자원/상태**에 접근할 수 있도록 하는 확장된 모델  
> Shared-state concurrency is an extended programming model  

* **특별한 Lock 메커니즘 필요** (Transaction 이나 Thread/State 이런 거 생각하면 됨)  
  > 안그러면, 불변식이 위반되거나 비결정론(일반적인 Concurrency)에 의해  
  > 일관성 / 상태 유효성 위반 관련된 문제를 야기함  
  
#### [말 참 어렵게 썼네](http://egloos.zum.com/aploit/v/5724750)  
> fundamental challenges of concurrency and concurrent programming  
> 스레드 세이프 관점이지만, 결국 동시성에 관련한 모든 이슈를 포함하고 있음.  
  
  
## Synchronization and Coordination as Concurrency Control  
> 프로그래밍 모델에 상관 없이 필요한 동시성에 대한 제어 (암시적, 명시적)  

#### 같은 공간에 대한 액세스 관리 없이, 복수의 실행 플로우는 항상 이슈  
  ```
  Two or more activities might access the same data 
  and thus induce data corruption 
  as well as inconsistent or invalid application state
  ```  

### Two Mechanisms attempting to tackle Concurrency Issue  
> **Synchronization**  
> **Coordination**  

#### Synchronization  
> more precisely **competition synchronization**  
* 경합 동기화 : 복수의 플로우들의 공유 자원 접근에 대한 제어  
* 공유 자원에 **동시 접근하면 안될 때** 특히 더 중요  
* **배타적 접근** 등의 다양한 방법을 사용해, 복수의 액세스(플로우에 의한)를 순차화  
* semaphore, lock 이런 개념들  

#### Coordination  
> **cooperation synchronization**  
* 협력 동기화 : 협력(의존) 하는 대상들 간의 작업 처리 상태 등을 동기화  
* CQRS 에서의 Command, Query 각각 격리된 데이터 자원들을 생각해보면 될 듯  
* 아니면 Master / Replication 데이터 변경 시 동기화도 좋은 예고..   

#### 동기화와 조정에 대한 부분은 주로 세트로 관리  
> 암/명시적인 방법을 섞어서  

* Implicit Synchronization  
  > 프로그래밍 언어/환경의 실행 메커니즘/체계 일부에 숨어있음  
  > 직접적인 제어 코드가 있는 게 아님  
  > 환경 자체가 공유하는 상태를 가질 수 없는 체계거나    
  > Immutable한 것들만 다루는 체계라면...  
  > 암시적으로 동기화 관련 이슈가 제어되고 있는 거임  
  > **암시적으로 제어되는 부분에 대한 이해가 필요해지고, 러닝 커브가 높을 수 있음**  
  
* Explicit Synchronization  
  > 프로그래머가 명시적으로 동기화 작업을 추가  
  > 필요한 부분에만 적용을 할 수가 있음  
  > ex] transaction 처리를 위한 코드  
    >> isolation level 에 따라서, 필요한 만큼만 적용   
  > 공유하는 상태를 동시에 접근할 수 없도록 하는 코드  
    >> 걍 단적으로 synchronized 이런 것들  
  > **문제에 따라서 배보다 배꼽이 더 커질 수 있음**  
    >> 비즈니스/도메인 문제보다 더 복잡한 문제가 될 수도 있음  
  
  
## Tasks, Processes and Threads  
> 동시성(Concurrency) 관련 엔티티들 소개  
> 운영체제에서 사용/제공 하는 실제 엔티티들  
> 해당 엔티티들과 하드웨어 간 상호 작용에 대한 매핑  

### Concurrent Task Execution  
* Orthogonal concepts for the concurrent execution of multiple tasks.  
  * : Task as a **general abstraction for a unit of execution**  
     ![](http://berb.github.io/diploma-thesis/community/resources/conc_exec.svg)  
     
#### MultiTasking  
> The ability to execute multiple tasks concurrently has been a crucial requirement for OS.  
> How to manage an interleaved and alternating execution of tasks.  

#### [Key Concept] Scheduling  
> organizes the assignment of processing time to tasks using  
> **Scheduling Strategies**.  

* In case of multiple CPU cores or CPUs  
  > multitasking can be complemented by **multiprocessing**  
  > allocates different tasks on available cores/CPUs.  

* Ex : **Scheduling Strategy** / Different Aims   
  * **fair scheduling**  
    > between tasks / fixed latencies  
    > for tasks executions or maximum utilization    
    > 공평하게 분배하는 방식 (작업량/단계 기준일수도 있고, 시간 기준일 수도 있고)  
  * **Preemption model**  
    > 선점 기준을 정립  
    * 스케쥴러가 처리 시간을 task에 주고, task를 일시 중지 시킴    
    * task는 **선점**에 대한 아무런 권한이 없음  
  * **Cooperative model**  
    * task는 **다른 task가 실행**될 수 있도록 응답에 대한 대기 책임을 가짐  
  * Scheduling is an important duty of any OS.  
    > However, it is also noteworthy(주목해야 할)  
    > applications themselves can provide some kind of scheduling of their own internal tasks.  
  
* OS generally provide different types of tasks, processes and threads.  
  > 추상화 레벨이랑 목적 생각  
  * **Process**  
    > heavyweight tasks unit  
    > owns system resources such as memory and file descriptors  
  * **Threads**  
    > lightweight tasks units  
    > belong to a certain process.  
    > allocated within the same process's resources (share memory/file descriptors ...)  
    > creating threads is a relatively cheap operation compared to the creation of new processes.  
    > 더 낮은 레벨까지 내려가서 할당 받아올 필요가 없음  
    > 프로세스 레벨에서 할당받은 자원의 관점이므로  

#### Most concurrent applications make heavy use of multiple threads.  
> 하지만, 프로그래밍 언어가 **스레드 가용**에 대해 실제적/명시적 엔티티를 사용하진 않음.  
> 프로그래밍 언어 목적에 따라 스레드가 아닌 모델로 추상화할 수도 있음.  
> 프로그래밍 언어 플랫폼에 따라 **복수의 스레드들이 가용되는 모델을 추상화 했을 수도 있음**.  
> 하나의 스레드만 사용되는 모델로 추상화하거나...  

```
Instead, the execution environment might map other concurrency constructs 
of a programming language to actual threads at runtime. 
Similarly, the shared-state property of multithreading might be idiomatically hidden by the language.
```


